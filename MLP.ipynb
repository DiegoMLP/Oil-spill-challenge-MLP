{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author : Srinath\n",
    "# For cerebras challenge\n",
    "# Credits : FlorianMuellerklein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'invalid': 'ignore', 'over': 'ignore', 'under': 'ignore'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "np.seterr(all = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Takes a real input and computes its sigmoid\n",
    "    :param x : Real input\n",
    "    :type x : float\n",
    "    '''\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(y):\n",
    "    '''\n",
    "    Returns the derivative of a sigmoid, given the sigmoid value\n",
    "    :param y: Sigmoid of a real number\n",
    "    :type y: float\n",
    "    '''\n",
    "    return y * (1.0 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    '''\n",
    "    Returns ReLU of x\n",
    "    '''\n",
    "    return x * (x > 0)\n",
    "\n",
    "def drelu(x):\n",
    "    '''\n",
    "    Returns derivative of ReLU of x\n",
    "    '''\n",
    "    return 1. * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(w):\n",
    "    '''\n",
    "    Returns the softmax output\n",
    "    :param w: Input\n",
    "    '''\n",
    "    e = np.exp(w - np.amax(w))\n",
    "    dist = e / np.sum(e)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP_Classifier(object):\n",
    "    \"\"\"\n",
    "    Basic MultiLayer Perceptron (MLP) neural network with regularization and learning rate decay\n",
    "    Consists of three layers: input, hidden and output.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input, hidden, output, iterations = 50, learning_rate = 0.01, \n",
    "                l2_in = 0, l2_out = 0, momentum = 0, rate_decay = 0, \n",
    "                output_layer = 'softmax', verbose = True):\n",
    "        \"\"\"\n",
    "        :param input: number of input neurons\n",
    "        :param hidden: number of hidden neurons\n",
    "        :param output: number of output neurons\n",
    "        :param iterations: how many epochs\n",
    "        :param learning_rate: initial learning rate\n",
    "        :param output_layer: activation (transfer) function of the output layer\n",
    "        :param verbose: whether to spit out error rates while training\n",
    "        \"\"\"\n",
    "        # initialize parameters\n",
    "        self.iterations = iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_in = l2_in\n",
    "        self.l2_out = l2_out\n",
    "        self.momentum = momentum\n",
    "        self.rate_decay = rate_decay\n",
    "        self.verbose = verbose\n",
    "        self.output_activation = output_layer\n",
    "        \n",
    "        # initialize arrays\n",
    "        self.input = input + 1 # add 1 for bias node\n",
    "        self.hidden = hidden \n",
    "        self.output = output\n",
    "\n",
    "        # set up array of 1s for activations\n",
    "        self.ai = np.ones(self.input)\n",
    "        self.ah = np.ones(self.hidden)\n",
    "        self.ao = np.ones(self.output)\n",
    "\n",
    "        # create randomized weights\n",
    "        input_range = 1.0 / self.input ** (1/2)\n",
    "        self.wi = np.random.normal(loc = 0, scale = input_range, size = (self.input, self.hidden))\n",
    "        self.wo = np.random.uniform(size = (self.hidden, self.output)) / np.sqrt(self.hidden)\n",
    "        \n",
    "        self.ci = np.zeros((self.input, self.hidden))\n",
    "        self.co = np.zeros((self.hidden, self.output))\n",
    "\n",
    "    def feedForward(self, inputs):\n",
    "        \"\"\"\n",
    "        The feedforward algorithm loops over all the nodes in the hidden layer and\n",
    "        adds together all the outputs from the input layer.\n",
    "        :param inputs: input data\n",
    "        :return: updated activation output vector\n",
    "        \"\"\"\n",
    "        if len(inputs) != self.input-1:\n",
    "            raise ValueError('Wrong number of inputs!')\n",
    "\n",
    "        # input activations\n",
    "        self.ai[0:self.input -1] = inputs\n",
    "\n",
    "        # hidden activations\n",
    "        sum = np.dot(self.wi.T, self.ai)\n",
    "        self.ah = relu(sum)\n",
    "        \n",
    "        # output activations\n",
    "        sum = np.dot(self.wo.T, self.ah)\n",
    "        self.ao = softmax(sum)\n",
    "        \n",
    "        return self.ao\n",
    "\n",
    "    def backPropagate(self, targets):\n",
    "        \"\"\"\n",
    "        For the output layer\n",
    "        1. Calculates the difference between output value and target value\n",
    "        2. Get the derivative (slope) of the sigmoid function in order to determine how much the weights need to change\n",
    "        3. update the weights for every node based on the learning rate and sig derivative\n",
    "        \n",
    "        For the hidden layer\n",
    "        1. calculate the sum of the strength of each output link multiplied by how much the target node has to change\n",
    "        2. get derivative to determine how much weights need to change\n",
    "        3. change the weights based on learning rate and derivative\n",
    "        \n",
    "        :param targets: y values\n",
    "        :param N: learning rate\n",
    "        :return: updated weights\n",
    "        \"\"\"\n",
    "        if len(targets) != self.output:\n",
    "            raise ValueError('Wrong number of targets you silly goose!')\n",
    "\n",
    "        # calculate error terms for output\n",
    "        output_deltas = -(targets - self.ao)\n",
    "        \n",
    "        # calculate error terms for hidden\n",
    "        # delta (theta) tells you which direction to change the weights\n",
    "        error = np.dot(self.wo, output_deltas)\n",
    "        hidden_deltas = drelu(self.ah) * error\n",
    "        \n",
    "        # update the weights connecting hidden to output, change == partial derivative\n",
    "        change = output_deltas * np.reshape(self.ah, (self.ah.shape[0],1))\n",
    "        regularization = self.l2_out * self.wo\n",
    "        self.wo -= self.learning_rate * (change + regularization) + self.co * self.momentum \n",
    "        self.co = change \n",
    "\n",
    "        # update the weights connecting input to hidden, change == partial derivative\n",
    "        change = hidden_deltas * np.reshape(self.ai, (self.ai.shape[0], 1))\n",
    "        regularization = self.l2_in * self.wi\n",
    "        self.wi -= self.learning_rate * (change + regularization) + self.ci * self.momentum \n",
    "        self.ci = change\n",
    "\n",
    "        # calculate error\n",
    "        error = -sum(targets * np.log(self.ao))\n",
    "        \n",
    "        return error\n",
    "\n",
    "    def test(self, patterns):\n",
    "        \"\"\"\n",
    "        Currently this will print out the targets next to the predictions.\n",
    "        \"\"\"\n",
    "        for p in patterns:\n",
    "            print(p[1], '->', self.feedForward(p[0]))\n",
    "\n",
    "    def fit(self, patterns):\n",
    "        if self.verbose == True:\n",
    "            print 'Using softmax activation in output layer'\n",
    "      \n",
    "        num_example = np.shape(patterns)[0]\n",
    "                \n",
    "        for i in range(self.iterations):\n",
    "            error = 0.0\n",
    "            random.shuffle(patterns)\n",
    "            for p in patterns:\n",
    "                inputs = p[0]\n",
    "                targets = p[1]\n",
    "                self.feedForward(inputs)\n",
    "                error += self.backPropagate(targets)\n",
    "            \n",
    "            if i % 10 == 0 and self.verbose == True:\n",
    "                error = error/num_example\n",
    "                print('Training error %-.5f' % error)\n",
    "                \n",
    "            # learning rate decay\n",
    "            self.learning_rate = self.learning_rate * (self.learning_rate / (self.learning_rate + (self.learning_rate * self.rate_decay)))\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        return list of predictions after training algorithm\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for p in X:\n",
    "            predictions.append(self.feedForward(p))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = np.loadtxt('Data/mnist_train.csv', delimiter = ',')\n",
    "\n",
    "    # first ten values are the one hot encoded y (target) values\n",
    "\n",
    "    temp = np.array(data[:,0],dtype=np.int)\n",
    "    y = np.zeros((data.shape[0],10))\n",
    "    y[np.arange(data.shape[0]), temp]=1\n",
    "\n",
    "    data = data[:,1:] # x data\n",
    "    data = scale(data)\n",
    "    out = []\n",
    "    \n",
    "    # populate the tuple list with the data\n",
    "    for i in range(data.shape[0]):\n",
    "        tupledata = list((data[i,:].tolist(), y[i].tolist()))\n",
    "        out.append(tupledata)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using softmax activation in output layer\n"
     ]
    }
   ],
   "source": [
    "X = load_data()\n",
    "NN = MLP_Classifier(784, 40000, 10, iterations = 50, learning_rate = 0.01, \n",
    "                    momentum = 0.5, rate_decay = 0.0001, \n",
    "                    output_layer = 'softmax')\n",
    "NN.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
